{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nSemantic Search Example [REST API]\n----------------------------------\n\nAn example of Semantic Search\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from __future__ import print_function\n\nimport os.path\nimport requests\nimport pandas as pd\n\npd.options.display.float_format = '{:,.3f}'.format\npd.options.display.expand_frame_repr = False\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n\nif __name__ == '__main__':\n\n    print(\" 0. Load the test dataset\")\n    url = BASE_URL + '/example-dataset/{}'.format(dataset_name)\n    print(\" GET\", url)\n    input_ds = requests.get(url).json()\n\n    # create a custom dataset definition for ingestion\n    data_dir = input_ds['metadata']['data_dir']\n    dataset_definition = [{'document_id': row['document_id'],\n                           'file_path': os.path.join(data_dir, row['file_path'])} \\\n                                   for row in input_ds['dataset']]\n\n    # 1. Feature extraction\n\n    print(\"\\n1.a Load dataset and initalize feature extraction\")\n    url = BASE_URL + '/feature-extraction'\n    print(\" POST\", url)\n    res = requests.post(url, json={'dataset_definition': dataset_definition,\n                                   'use_hashing': True}).json()\n\n    dsid = res['id']\n    print(\"   => received {}\".format(list(res.keys())))\n    print(\"   => dsid = {}\".format(dsid))\n\n    print(\"\\n1.b Start feature extraction\")\n\n    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n    print(\" POST\", url)\n    requests.post(url)\n\n    # 3. Document categorization with LSI (used for Nearest Neighbors method)\n\n    print(\"\\n2. Calculate LSI\")\n\n    url = BASE_URL + '/lsi/'\n    print(\"POST\", url)\n\n    n_components = 100\n    res = requests.post(url,\n                        json={'n_components': n_components,\n                              'parent_id': dsid\n                              }).json()\n\n    lsi_id = res['id']\n    print('  => LSI model id = {}'.format(lsi_id))\n    print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(\n                            n_components, res['explained_variance']*100))\n\n\n    # 3. Semantic search\n\n    print(\"\\n3.a. Perform the semantic search\")\n\n\n    query = \"\"\"There are some conflicts with the draft date, so we will probably need to\n                have it on a different date.\"\"\"\n\n    url = BASE_URL + '/search/'\n    print(\" POST\", url)\n\n    res = requests.post(url,\n                        json={'parent_id': lsi_id,\n                              'query': query\n                              }).json()\n\n    data = res['data']\n\n    df = pd.DataFrame(data).set_index('internal_id')\n    print(df)\n\n    print(df.score.max())\n\n\n    # 4. Cleaning\n    print(\"\\n4. Delete the extracted features\")\n    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n    print(\" DELETE\", url)\n    requests.delete(url)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}