{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "freediscovery-env     *  /Users/kcom/anaconda/envs/freediscovery-env\r\n",
      "go_r                     /Users/kcom/anaconda/envs/go_r\r\n",
      "topik                    /Users/kcom/anaconda/envs/topik\r\n",
      "root                     /Users/kcom/anaconda\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda info -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from time import time, sleep\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import freediscovery.tests as ft\n",
    "#import freediscovery as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parent_dir(path, n=0):\n",
    "    path = os.path.abspath(path)\n",
    "    if n==0:\n",
    "        return path\n",
    "    else:\n",
    "        return os.path.dirname(_parent_dir(path, n=n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _print_url(op, url):\n",
    "    print(' '*1, op, url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6e8f250fd97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# normal setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_dir_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parent_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#data_dir_l = os.path.join(data_dir_l, \"freediscovery_shared\", \"tar_fd_benchmark\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# normal setup\n",
    "data_dir_l = _parent_dir(__file__, n=3)\n",
    "data_dir_l = os.path.join(data_dir_l, \"freediscovery_shared\", \"tar_fd_benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# docker setip\n",
    "data_dir_d = \"/freediscovery_shared/tar_fd_benchmark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\n",
    "\"\"\"## =========================================================== ##\n",
    "##                                                             ##\n",
    "##    FREEDiscovery categorization example (Python)            ##\n",
    "##                                                             ##\n",
    "## Note that this only illustrates part of the implemented API.##\n",
    "## =========================================================== ##\\n\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\"\" This example uses a 37k documents subset of the TREC 2009 CORPUS (201)\"\"\")\n",
    "\n",
    "#BASE_URL = \"http://localhost:5001\"  # FREEDiscovery server URL\n",
    "BASE_URL = \"http://52.38.241.62:5001\"  # FREEDiscovery server URL\n",
    "BASE_URL += '/api/v0'\n",
    "dsid = None                               # set the dataset id here,\n",
    "                                          # otherwise new feature generation will be executed\n",
    "\n",
    "#dsid = \"bdfa3179d0d24f9788144241973b16d1\"\n",
    "if 'localhost' in BASE_URL:\n",
    "    data_dir_d = data_dir_l\n",
    "data_dir = data_dir_d\n",
    "\n",
    "\n",
    "if dsid is None:\n",
    "# 1. Feature extracition\n",
    "\n",
    "    print(\"\\n1.a Load dataset and initalize feature extraction\")\n",
    "    url = BASE_URL + '/feature-extraction'\n",
    "    _print_url(\"POST\", url)\n",
    "    res = requests.post(url,\n",
    "            json={'data_dir': os.path.join(data_dir, \"data\"),\n",
    "                'n_features': 100000, 'analyzer': 'word',\n",
    "                'ngram_range': (1, 1), 'stop_words': 'english',\n",
    "                'chunk_size': 5000, 'n_jobs': 4, 'use_idf': 1,\n",
    "                'sublinear_tf': 0, 'binary': 0})\n",
    "\n",
    "    dsid = res.json()['id']\n",
    "    print(\"   => received {}\".format(list(res.json().keys())))\n",
    "    print(\"   => dsid = {}\".format(dsid))\n",
    "\n",
    "    print(\"\\n1.b Start feature extraction (non blocking)\")\n",
    "\n",
    "    # Make this call non blocking\n",
    "    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n",
    "    _print_url(\"POST\", url)\n",
    "    p = Process(target=requests.post, args=(url,))\n",
    "    p.start()\n",
    "    sleep(5.0) # wait a bit for the processing to start\n",
    "\n",
    "    print('\\n1.c Monitor feature extraction progress')\n",
    "    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n",
    "    _print_url(\"GET\", url)\n",
    "\n",
    "    t0 = time()\n",
    "    while True:\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 520:\n",
    "            p.terminate()\n",
    "            raise ValueError('Processing did not start')\n",
    "        elif res.status_code == 200:\n",
    "            break # processing finished\n",
    "        data = res.json()\n",
    "        print('     ... {}k/{}k files processed in {:.1f} min'.format(\n",
    "                    data['n_samples_processed']//1000, data['n_samples']//1000, (time() - t0)/60.))\n",
    "        sleep(15.0)\n",
    "\n",
    "    p.terminate() # just in case, should not be necessary\n",
    "\n",
    "\n",
    "print(\"\\n1.d. check the parameters of the extracted features\")\n",
    "url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n",
    "_print_url('GET', url)\n",
    "res = requests.get(url)\n",
    "\n",
    "data = res.json()\n",
    "for key, val in data.items():\n",
    "    if key!='filenames':\n",
    "        print('     - {}: {}'.format(key, val))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n2.a. Load relevant & non relevant seed file list\") \n",
    "with open(os.path.join(data_dir_l,'seed_relevant.txt'), 'rt') as fh:\n",
    "    relevant_files = [el.strip() for el in fh.readlines()]\n",
    "\n",
    "with open(os.path.join(data_dir_l,'seed_non_relevant.txt'), 'rt') as fh:\n",
    "    non_relevant_files = [el.strip() for el in fh.readlines()]\n",
    "\n",
    "\n",
    "print(\"\\n3.b. Train the categorization model\")\n",
    "print(\"       {} relevant, {} non-relevant files\".format(\n",
    "    len(relevant_files), len(non_relevant_files)))\n",
    "url = BASE_URL + '/categorization-model/'\n",
    "_print_url(\"POST\", url)\n",
    "\n",
    "res = requests.post(url,\n",
    "        json={'relevant_filenames': relevant_files,\n",
    "              'non_relevant_filenames': non_relevant_files,\n",
    "              'dataset_id': dsid,\n",
    "              'method': 'LogisticRegression', # one of \"LinearSVC\", \"LogisticRegression\", \"LogisticRegressionCV\", \"SGDClassifier\"\n",
    "              'training_scores': 1\n",
    "              }) \n",
    "\n",
    "data = res.json()\n",
    "mid  = data['id']\n",
    "print(\"     => model id = {}\".format(mid))\n",
    "print('     => Training scores: F1 score = {:.2f},   recall = {:.2f},   precision = {:.2f}'.format(\n",
    "    data['F1_score'], data['recall_score'], data['precision_score'], ))\n",
    "\n",
    "print(\"\\n3.c. Check the parameters used in the categorization model\")\n",
    "url = BASE_URL + '/categorization-model/{}'.format(mid)\n",
    "_print_url(\"GET\", url)\n",
    "res = requests.get(url)\n",
    "\n",
    "data = res.json()\n",
    "for key, val in data.items():\n",
    "    if \"filenames\" not in key:\n",
    "        print('     - {}: {}'.format(key, val))\n",
    "\n",
    "threshold = 0.0\n",
    "\n",
    "print(\"\\n3.d Categorize the complete dataset with this model\")\n",
    "url = BASE_URL + '/categorization-model/{}/predict'.format(mid)\n",
    "_print_url(\"GET\", url)\n",
    "res = requests.get(url)\n",
    "prediction = res.json()['prediction']\n",
    "\n",
    "print(\"    => Predicting {} relevant and {} non relevant documents\".format(\n",
    "    len(list(filter(lambda x: x>threshold, prediction))),\n",
    "    len(list(filter(lambda x: x<threshold, prediction)))))\n",
    "\n",
    "print(\"\\n3.e Test categorization accuracy\")\n",
    "gtfile = os.path.join(data_dir, \"ground_truth_file.txt\")\n",
    "print(\"         using {}\".format(gtfile))  \n",
    "url = BASE_URL + '/categorization-model/{}/test'.format(mid)\n",
    "_print_url(\"POST\", url)\n",
    "res = requests.post(url,\n",
    "        json={'ground_truth_filename': gtfile})\n",
    "               \n",
    "data = res.json()\n",
    "print('     => Test scores: F1 score = {:.2f},   recall = {:.2f},   precision = {:.2f}'.format(\n",
    "    data['F1_score'], data['recall_score'], data['precision_score'], ))\n",
    "\n",
    "print(\"\\n4.a. Calculate LSI\")\n",
    "\n",
    "url = BASE_URL + '/lsi/'\n",
    "_print_url(\"POST\", url)\n",
    "\n",
    "n_components = 100\n",
    "res = requests.post(url,\n",
    "        json={ 'n_components': n_components,\n",
    "              'dataset_id': dsid,\n",
    "              }) \n",
    "\n",
    "data = res.json()\n",
    "lid  = data['id']\n",
    "print('  => LSI model id = {}'.format(lid))\n",
    "print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(\n",
    "                        n_components, data['explained_variance']*100))\n",
    "print(\"\\n4.b. Predict categorization with LSI\")\n",
    "\n",
    "url = BASE_URL + '/lsi/{}/predict'.format(lid)\n",
    "_print_url(\"POST\", url)\n",
    "res = requests.post(url,\n",
    "        json={'relevant_filenames': relevant_files,\n",
    "              'non_relevant_filenames': non_relevant_files,\n",
    "              'threshold': 'auto'\n",
    "              }) \n",
    "data = res.json()\n",
    "prediction = data['prediction']\n",
    "\n",
    "print('     => Training scores: F1 score = {:.2f},   recall = {:.2f},   precision = {:.2f}'.format(\n",
    "    data['F1_score'], data['recall_score'], data['precision_score'], ))\n",
    "print(\"    => Predicting {} relevant and {} non relevant documents\".format(\n",
    "    len(list(filter(lambda x: x>threshold, prediction))),\n",
    "    len(list(filter(lambda x: x<threshold, prediction)))))\n",
    "\n",
    "print(\"\\n4.c. Test categorization with LSI\")\n",
    "url = BASE_URL + '/lsi/{}/test'.format(lid)\n",
    "_print_url(\"POST\", url)\n",
    "\n",
    "res = requests.post(url,\n",
    "        json={'relevant_filenames': relevant_files,\n",
    "              'non_relevant_filenames': non_relevant_files,\n",
    "              'threshold': 'auto',\n",
    "              'ground_truth_filename': gtfile\n",
    "              }) \n",
    "data = res.json()\n",
    "print('     => Test scores: F1 score = {:.2f},   recall = {:.2f},   precision = {:.2f}'.format(\n",
    "    data['F1_score'], data['recall_score'], data['precision_score'], ))\n",
    "\n",
    "\n",
    "print(\"\\n5.a Delete the extracted features (not actually calling this)\")\n",
    "url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n",
    "_print_url(\"DELETE\", url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:freediscovery-env]",
   "language": "python",
   "name": "conda-env-freediscovery-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
