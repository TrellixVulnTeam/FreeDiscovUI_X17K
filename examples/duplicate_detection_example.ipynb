{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Duplicate Detection Example [REST API]\n",
    "--------------------------------------\n",
    "\n",
    "Find near-duplicates in a text collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. Load the test dataset\n",
      " POST http://localhost:5001/api/v0/datasets/treclegal09_2k_subset\n",
      "\n",
      "1.a Load dataset and initalize feature extraction\n",
      " POST http://localhost:5001/api/v0/feature-extraction\n",
      "   => received ['id', 'filenames']\n",
      "   => dsid = c40ded114385447b9465617ed3ec9351\n",
      "\n",
      "1.b Run feature extraction\n",
      " POST http://localhost:5001/api/v0/feature-extraction/c40ded114385447b9465617ed3ec9351\n",
      "\n",
      "1.d. check the parameters of the extracted features\n",
      " GET http://localhost:5001/api/v0/feature-extraction/c40ded114385447b9465617ed3ec9351\n",
      "     - data_dir: /shared/code/wking_code/freediscovery_shared/treclegal09_2k_subset/data\n",
      "     - use_idf: True\n",
      "     - analyzer: word\n",
      "     - norm: l2\n",
      "     - max_df: 0.75\n",
      "     - use_hashing: False\n",
      "     - chunk_size: 2000\n",
      "     - n_samples: 2465\n",
      "     - min_df: 4.0\n",
      "     - n_features: 30001\n",
      "     - stop_words: english\n",
      "     - sublinear_tf: False\n",
      "     - binary: False\n",
      "     - ngram_range: [1, 1]\n",
      "     - n_jobs: -1\n",
      "     - n_samples_processed: 2465\n",
      "\n",
      "2. Near Duplicates detection by cosine similarity (DBSCAN)\n",
      " POST http://localhost:5001/api/v0/clustering/dbscan/\n",
      "     => model id = 37b9df53a11443c8a5794e8e8d9d39e3\n",
      " POST http://localhost:5001/api/v0/clustering/dbscan/37b9df53a11443c8a5794e8e8d9d39e3\n",
      "    .. computed in 3.7s\n",
      "Found 91 duplicates / 2465\n",
      "\n",
      "3. Near Duplicates Detection using I-Match\n",
      " POST http://localhost:5001/api/v0/duplicate-detection/\n",
      "     => model id = fd56381266584d4ea682888ab16e2222\n",
      "    .. computed in 0.0s\n",
      "GET http://localhost:5001/api/v0/duplicate-detection/fd56381266584d4ea682888ab16e2222\n",
      "    .. computed in 2.8s\n",
      "Found 336 duplicates / 2465\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "\n",
    "dataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n",
    "\n",
    "BASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n",
    "\n",
    "print(\" 0. Load the test dataset\")\n",
    "url = BASE_URL + '/datasets/{}'.format(dataset_name)\n",
    "print(\" POST\", url)\n",
    "res = requests.get(url)\n",
    "res = res.json()\n",
    "\n",
    "# To use a custom dataset, simply specify the following variables\n",
    "data_dir = res['data_dir']\n",
    "# # 1. Feature extraction (non hashed)\n",
    "\n",
    "print(\"\\n1.a Load dataset and initalize feature extraction\")\n",
    "url = BASE_URL + '/feature-extraction'\n",
    "print(\" POST\", url)\n",
    "fe_opts = {'data_dir': data_dir,\n",
    "           'stop_words': 'english', 'chunk_size': 2000, 'n_jobs': -1,\n",
    "           'use_idf': 1, 'sublinear_tf': 0, 'binary': 0, 'n_features': 30001,\n",
    "           'analyzer': 'word', 'ngram_range': (1, 1), \"norm\": \"l2\",\n",
    "           'use_hashing': False,  # hashing should be disabled for clustering\n",
    "           'min_df': 4, 'max_df': 0.75\n",
    "          }\n",
    "res = requests.post(url, json=fe_opts)\n",
    "\n",
    "dsid = res.json()['id']\n",
    "print(\"   => received {}\".format(list(res.json().keys())))\n",
    "print(\"   => dsid = {}\".format(dsid))\n",
    "\n",
    "\n",
    "print(\"\\n1.b Run feature extraction\")\n",
    "# progress status is available for the hashed version only\n",
    "url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n",
    "print(\" POST\", url)\n",
    "res = requests.post(url)\n",
    "\n",
    "print(\"\\n1.d. check the parameters of the extracted features\")\n",
    "url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n",
    "print(' GET', url)\n",
    "res = requests.get(url)\n",
    "\n",
    "data = res.json()\n",
    "print('\\n'.join(['     - {}: {}'.format(key, val) for key, val in data.items() \\\n",
    "                                                  if \"filenames\" not in key]))\n",
    "\n",
    "\n",
    "# # 2. Near Duplicates detection by cosine similarity (DBSCAN)\n",
    "\n",
    "print(\"\\n2. Near Duplicates detection by cosine similarity (DBSCAN)\")\n",
    "\n",
    "url = BASE_URL + '/clustering/dbscan/'\n",
    "print(\" POST\", url)\n",
    "t0 = time()\n",
    "res = requests.post(url,\n",
    "        json={'dataset_id': dsid,\n",
    "              'lsi_components': 100,\n",
    "              'eps': 0.1,            # 2*cosine distance for documents to be considered as duplicates\n",
    "              'n_max_samples': 2\n",
    "              }).json()\n",
    "\n",
    "mid  = res['id']\n",
    "print(\"     => model id = {}\".format(mid))\n",
    "\n",
    "url = BASE_URL + '/clustering/dbscan/{}'.format(mid)\n",
    "print(\" POST\", url)\n",
    "res = requests.get(url,\n",
    "        json={'n_top_words': 0, # don't compute cluster labels\n",
    "              }).json()\n",
    "t1 = time()\n",
    "\n",
    "print('    .. computed in {:.1f}s'.format(t1 - t0))\n",
    "\n",
    "labels_ = res['labels']\n",
    "\n",
    "print('Found {} duplicates / {}'.format(len(labels_) - len(np.unique(labels_)), len(labels_)))\n",
    "\n",
    "print(\"\\n3. Near Duplicates Detection using I-Match\")\n",
    "\n",
    "url = BASE_URL + '/duplicate-detection/'\n",
    "print(\" POST\", url)\n",
    "t0 = time()\n",
    "res = requests.post(url,\n",
    "        json={'dataset_id': dsid,\n",
    "              'method': 'i-match',\n",
    "              }) \n",
    "\n",
    "data = res.json()\n",
    "mid  = data['id']\n",
    "print(\"     => model id = {}\".format(mid))\n",
    "\n",
    "print('    .. computed in {:.1f}s'.format(time() - t0))\n",
    "\n",
    "\n",
    "url = BASE_URL + '/duplicate-detection/{}'.format(mid)\n",
    "print(\"GET\", url)\n",
    "t0 = time()\n",
    "res = requests.get(url,\n",
    "        json={'n_rand_lexicons': 10,\n",
    "              'rand_lexicon_ratio': 0.9}).json()\n",
    "t1 = time()\n",
    "print('    .. computed in {:.1f}s'.format(time() - t0))\n",
    "\n",
    "labels_ = res['cluster_id']\n",
    "\n",
    "print('Found {} duplicates / {}'.format(len(labels_) - len(np.unique(labels_)), len(labels_)))\n",
    "\n",
    "\n",
    "# Commenting out simhash for the moment\n",
    "# \n",
    "#if platform.system() == 'Windows':\n",
    "#    print('Simhash-py is currently not installed on Windows')\n",
    "#    sys.exit()\n",
    "#\n",
    "#print(\"\\n3. Duplicate detection by Simhash\")\n",
    "#\n",
    "#url = BASE_URL + '/duplicate-detection/'\n",
    "#print(\" POST\", url)\n",
    "#t0 = time()\n",
    "#res = requests.post(url,\n",
    "#        json={'dataset_id': dsid,\n",
    "#              'method': 'simhash',\n",
    "#              }) \n",
    "#\n",
    "#data = res.json()\n",
    "#mid  = data['id']\n",
    "#print(\"     => model id = {}\".format(mid))\n",
    "#\n",
    "#print('    .. computed in {:.1f}s'.format(time() - t0))\n",
    "#\n",
    "#\n",
    "#\n",
    "#url = BASE_URL + '/duplicate-detection/{}'.format(mid)\n",
    "#print(\" GET\", url)\n",
    "#t0 = time()\n",
    "#res = requests.get(url,\n",
    "#        json={'distance': 1 }) \n",
    "#data = res.json()\n",
    "#print('    .. computed in {:.1f}s'.format(time() - t0))\n",
    "#\n",
    "#labels_ = data['cluster_id']\n",
    "#\n",
    "#print('Found {} duplicates / {}'.format(len(labels_) - len(np.unique(labels_)), len(labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:freediscovery-env]",
   "language": "python",
   "name": "conda-env-freediscovery-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
