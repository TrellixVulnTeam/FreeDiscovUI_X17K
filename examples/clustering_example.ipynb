{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Clustering Example [REST API]\n",
    "-----------------------------\n",
    "\n",
    "Cluster documents into clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. Load the test dataset\n",
      " POST http://localhost:5001/api/v0/datasets/treclegal09_2k_subset\n",
      "\n",
      "1.a Load dataset and initalize feature extraction\n",
      " POST http://localhost:5001/api/v0/feature-extraction\n",
      "   => received ['filenames', 'id']\n",
      "   => dsid = 906d6a5e4b634fb882bd64d0d975e66f\n",
      "\n",
      "1.b Run feature extraction\n",
      " POST http://localhost:5001/api/v0/feature-extraction/906d6a5e4b634fb882bd64d0d975e66f\n",
      "\n",
      "1.d. check the parameters of the extracted features\n",
      " GET http://localhost:5001/api/v0/feature-extraction/906d6a5e4b634fb882bd64d0d975e66f\n",
      "     - n_features: 30001\n",
      "     - max_df: 0.75\n",
      "     - ngram_range: [1, 1]\n",
      "     - chunk_size: 2000\n",
      "     - sublinear_tf: True\n",
      "     - n_samples_processed: 2465\n",
      "     - analyzer: word\n",
      "     - stop_words: english\n",
      "     - binary: False\n",
      "     - norm: l2\n",
      "     - use_hashing: False\n",
      "     - n_samples: 2465\n",
      "     - use_idf: True\n",
      "     - n_jobs: -1\n",
      "     - data_dir: /shared/code/wking_code/freediscovery_shared/treclegal09_2k_subset/data\n",
      "     - min_df: 4.0\n",
      "\n",
      "2.a. Document clustering (LSI + K-means)\n",
      " POST http://localhost:5001/api/v0/clustering/k-mean/\n",
      "     => model id = 7f19bf164a4a47408519e3bebcc3e964\n",
      "\n",
      "2.b. Computing cluster labels\n",
      " POST http://localhost:5001/api/v0/clustering/k-mean/7f19bf164a4a47408519e3bebcc3e964\n",
      "    .. computed in 2.1s\n",
      "   N_documents                                      cluster_names\n",
      "4          486  ['enron', 'energy', 'trading', 'services', 'co...\n",
      "3          482  ['shackleton', 'test', 'recipients', 'group', ...\n",
      "2          425      ['tenet', 'test', 'oct', 'nov', 'tue', 'wed']\n",
      "5          311  ['ect', 'hou', 'nemec', 'shackleton', 'enron_d...\n",
      "1          225  ['ect', 'recipients', 'group', 'haedicke', 'ad...\n",
      "9          178  ['teneo', 'recipients', 'administrative', 'ric...\n",
      "0          135  ['shall', 'party', 'agreement', 'transaction',...\n",
      "7          102  ['sanders', 'nov', 'ect', 'test', 'meeting', '...\n",
      "8           64  ['migration', 'outlook', 'team', 'mtg', 'oct',...\n",
      "6           57  ['rewrite', 'server', 'address', 'smtp', 'mail...\n",
      "\n",
      "2.a. Document clustering (LSI + Ward HC)\n",
      " POST http://localhost:5001/api/v0/clustering/ward_hc/\n",
      "     => model id = 1cbfeea563c7431d8c17072f8e65b84a\n",
      "\n",
      "2.b. Computing cluster labels\n",
      "POST http://localhost:5001/api/v0/clustering/ward_hc/1cbfeea563c7431d8c17072f8e65b84a\n",
      "    .. computed in 3.4s\n",
      "   N_documents                                      cluster_names\n",
      "5          443      ['tenet', 'test', 'oct', 'nov', 'tue', 'mon']\n",
      "1          423  ['recipients', 'administrative', 'group', 'tes...\n",
      "2          398  ['enron', 'energy', 'power', 'trade', 'market'...\n",
      "0          393  ['ect', 'hou', 'tana', 'group', 'recipients', ...\n",
      "6          342  ['shackleton', 'ect', 'test', 'group', 'recipi...\n",
      "4          166  ['shall', 'party', 'agreement', 'transaction',...\n",
      "8           95  ['sanders', 'nov', 'ect', 'test', 'lunch', 'me...\n",
      "3           85  ['enron_development', 'ect', 'shackleton', 'ho...\n",
      "9           64  ['migration', 'outlook', 'team', 'mtg', 'oct',...\n",
      "7           56  ['rewrite', 'server', 'address', 'smtp', 'mail...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import requests\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "\n",
    "def repr_clustering(labels, terms):\n",
    "    out = []\n",
    "    for ridx, row in enumerate(terms):\n",
    "        out.append({'cluster_names': row, 'N_documents': (labels == ridx).sum()})\n",
    "    out = pd.DataFrame(out).sort_values('N_documents', ascending=False)\n",
    "    return out\n",
    "\n",
    "dataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n",
    "\n",
    "BASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n",
    "\n",
    "print(\" 0. Load the test dataset\")\n",
    "url = BASE_URL + '/datasets/{}'.format(dataset_name)\n",
    "print(\" POST\", url)\n",
    "res = requests.get(url).json()\n",
    "\n",
    "# To use a custom dataset, simply specify the following variables\n",
    "data_dir = res['data_dir']\n",
    "\n",
    "# # 1. Feature extraction (non hashed)\n",
    "\n",
    "print(\"\\n1.a Load dataset and initalize feature extraction\")\n",
    "url = BASE_URL + '/feature-extraction'\n",
    "print(\" POST\", url)\n",
    "fe_opts = {'data_dir': data_dir,\n",
    "           'stop_words': 'english', 'chunk_size': 2000, 'n_jobs': -1,\n",
    "           'use_idf': 1, 'sublinear_tf': 1, 'binary': 0, 'n_features': 30001,\n",
    "           'analyzer': 'word', 'ngram_range': (1, 1), \"norm\": \"l2\",\n",
    "           'use_hashing': False,  # hashing should be disabled for clustering\n",
    "           'min_df': 4, 'max_df': 0.75\n",
    "           }\n",
    "res = requests.post(url, json=fe_opts).json()\n",
    "\n",
    "dsid = res['id']\n",
    "print(\"   => received {}\".format(list(res.keys())))\n",
    "print(\"   => dsid = {}\".format(dsid))\n",
    "\n",
    "\n",
    "print(\"\\n1.b Run feature extraction\")\n",
    "# progress status is available for the hashed version only\n",
    "url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n",
    "print(\" POST\", url)\n",
    "res = requests.post(url)\n",
    "\n",
    "print(\"\\n1.d. check the parameters of the extracted features\")\n",
    "url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n",
    "print(' GET', url)\n",
    "res = requests.get(url).json()\n",
    "\n",
    "print('\\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \\\n",
    "                                                  if \"filenames\" not in key]))\n",
    "\n",
    "\n",
    "# # 2. Document Clustering (LSI + K-Means)\n",
    "\n",
    "print(\"\\n2.a. Document clustering (LSI + K-means)\")\n",
    "\n",
    "url = BASE_URL + '/clustering/k-mean/'\n",
    "print(\" POST\", url)\n",
    "t0 = time()\n",
    "res = requests.post(url,\n",
    "                    json={'dataset_id': dsid,\n",
    "                          'n_clusters': 10,\n",
    "                          'lsi_components': 50\n",
    "                          }).json()\n",
    "\n",
    "mid = res['id']\n",
    "print(\"     => model id = {}\".format(mid))\n",
    "\n",
    "print(\"\\n2.b. Computing cluster labels\")\n",
    "url = BASE_URL + '/clustering/k-mean/{}'.format(mid)\n",
    "print(\" POST\", url)\n",
    "res = requests.get(url,\n",
    "                   json={'n_top_words': 6\n",
    "                         }).json()\n",
    "t1 = time()\n",
    "\n",
    "print('    .. computed in {:.1f}s'.format(t1 - t0))\n",
    "print(repr_clustering(np.array(res['labels']), res['cluster_terms']))\n",
    "\n",
    "\n",
    "# # 3. Document Clustering (LSI + Ward Hierarchical Clustering)\n",
    "\n",
    "print(\"\\n2.a. Document clustering (LSI + Ward HC)\")\n",
    "\n",
    "url = BASE_URL + '/clustering/ward_hc/'\n",
    "print(\" POST\", url)\n",
    "t0 = time()\n",
    "res = requests.post(url,\n",
    "                    json={'dataset_id': dsid,\n",
    "                          'n_clusters': 10,\n",
    "                          'lsi_components': 50,\n",
    "                          'n_neighbors': 5  # this is the connectivity constraint\n",
    "                          }).json()\n",
    "\n",
    "mid = res['id']\n",
    "print(\"     => model id = {}\".format(mid))\n",
    "\n",
    "print(\"\\n2.b. Computing cluster labels\")\n",
    "url = BASE_URL + '/clustering/ward_hc/{}'.format(mid)\n",
    "print(\"POST\", url)\n",
    "res = requests.get(url,\n",
    "                   json={'n_top_words': 6\n",
    "                         }).json()\n",
    "t1 = time()\n",
    "\n",
    "print('    .. computed in {:.1f}s'.format(t1 - t0))\n",
    "print(repr_clustering(np.array(res['labels']), res['cluster_terms']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:freediscovery-env]",
   "language": "python",
   "name": "conda-env-freediscovery-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
