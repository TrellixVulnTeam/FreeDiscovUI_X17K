{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Categorization Example [REST API]\n",
    "---------------------------------\n",
    "\n",
    "An example to illustrate binary categorizaiton with FreeDiscovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import time, sleep\n",
    "from multiprocessing import Process\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.options.display.expand_frame_repr = False\n",
    "\n",
    "dataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n",
    "\n",
    "BASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n",
    "#BASE_URL = \"http://52.38.241.62:5001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. Load the test dataset\n",
      " POST http://localhost:5001/api/v0/datasets/treclegal09_2k_subset\n",
      "\n",
      "1.a Load dataset and initalize feature extraction\n",
      " POST http://localhost:5001/api/v0/feature-extraction\n",
      "   => received ['filenames', 'id']\n",
      "   => dsid = 3b1e20c376624a7b9b524796125c457a\n",
      "\n",
      "1.b Start feature extraction (in the background)\n",
      " POST http://localhost:5001/api/v0/feature-extraction/3b1e20c376624a7b9b524796125c457a\n",
      "\n",
      "1.c Monitor feature extraction progress\n",
      " GET http://localhost:5001/api/v0/feature-extraction/3b1e20c376624a7b9b524796125c457a\n",
      "\n",
      "1.d. check the parameters of the extracted features\n",
      " GET http://localhost:5001/api/v0/feature-extraction/3b1e20c376624a7b9b524796125c457a\n",
      "     - binary: False\n",
      "     - sublinear_tf: False\n",
      "     - min_df: 0.0\n",
      "     - n_jobs: -1\n",
      "     - use_hashing: True\n",
      "     - use_idf: True\n",
      "     - max_df: 1.0\n",
      "     - n_features: 50001\n",
      "     - n_samples: 2465\n",
      "     - stop_words: english\n",
      "     - data_dir: /shared/code/wking_code/freediscovery_shared/treclegal09_2k_subset/data\n",
      "     - n_samples_processed: 2465\n",
      "     - analyzer: word\n",
      "     - chunk_size: 2000\n",
      "     - norm: l2\n",
      "     - ngram_range: [1, 1]\n",
      "\n",
      "2.a. Train the ML categorization model\n",
      "       5 relevant, 63 non-relevant files\n",
      " POST http://localhost:5001/api/v0/categorization/\n",
      " Training...\n",
      "     => model id = d269a5f8cd904c0fb03aba9e1fff7ef5\n",
      "    => Training scores: MAP = 1.000, ROC-AUC = 1.000\n",
      "\n",
      "2.b. Check the parameters used in the categorization model\n",
      " GET http://localhost:5001/api/v0/categorization/d269a5f8cd904c0fb03aba9e1fff7ef5\n",
      "     - method: LinearSVC\n",
      "     - options: {'loss': 'squared_hinge', 'C': 1.0, 'class_weight': None, 'fit_intercept': True, 'dual': True, 'intercept_scaling': 1, 'verbose': 0, 'penalty': 'l2', 'multi_class': 'ovr', 'max_iter': 1000, 'random_state': None, 'tol': 0.0001}\n",
      "\n",
      "2.c Categorize the complete dataset with this model\n",
      " GET http://localhost:5001/api/v0/categorization/d269a5f8cd904c0fb03aba9e1fff7ef5/predict\n",
      "    => Predicting 11 relevant and 2454 non relevant documents\n",
      "\n",
      "2.d Test categorization accuracy\n",
      "         using /shared/code/wking_code/freediscovery_shared/treclegal09_2k_subset/ground_truth_file.txt\n",
      "POST http://localhost:5001/api/v0/categorization/d269a5f8cd904c0fb03aba9e1fff7ef5/test\n",
      "    => Test scores: MAP = 1.000, ROC-AUC = 1.000\n",
      "\n",
      "3.a. Calculate LSI\n",
      "POST http://localhost:5001/api/v0/lsi/\n",
      "  => LSI model id = 84a6a188929b44359dae4d5f72ec52f4\n",
      "  => SVD decomposition with 100 dimensions explaining 48.41 % variabilty of the data\n",
      "\n",
      "3.b. Predict categorization with LSI\n",
      "POST http://localhost:5001/api/v0/lsi/84a6a188929b44359dae4d5f72ec52f4/predict\n",
      "    => Training scores: MAP = 1.000, ROC-AUC = 1.000\n",
      "\n",
      "3.c. Test categorization with LSI\n",
      " POST http://localhost:5001/api/v0/lsi/84a6a188929b44359dae4d5f72ec52f4/test\n",
      "    => Test scores: MAP = 0.751, ROC-AUC = 0.822\n",
      "\n",
      "       nearest_nrel_doc  nearest_rel_doc  prediction\n",
      "0                   29                4      -0.414\n",
      "1                    9                4      -0.466\n",
      "2                   36                4      -0.568\n",
      "3                   26                1       1.000\n",
      "4                   36                1      -0.600\n",
      "5                   47                2      -0.502\n",
      "6                   30                1      -0.664\n",
      "7                   36                0      -0.421\n",
      "8                   47                1      -0.449\n",
      "9                   61                4      -1.000\n",
      "10                  10                1      -0.607\n",
      "11                  38                3      -0.367\n",
      "12                  38                3      -0.356\n",
      "13                  18                0      -0.841\n",
      "14                  18                0      -0.996\n",
      "15                  42                2      -0.334\n",
      "16                  38                1      -0.375\n",
      "17                  38                3      -0.246\n",
      "18                  45                2      -0.212\n",
      "19                   0                4      -0.520\n",
      "20                  47                2      -0.354\n",
      "21                  24                4      -0.895\n",
      "22                   2                1      -0.169\n",
      "23                   2                2      -0.171\n",
      "24                  22                0      -0.541\n",
      "25                  24                4      -0.569\n",
      "26                  22                3      -0.267\n",
      "27                  57                0      -0.324\n",
      "28                  38                1      -0.360\n",
      "29                  38                3      -0.239\n",
      "...                ...              ...         ...\n",
      "2435                14                2      -0.189\n",
      "2436                52                0      -0.726\n",
      "2437                52                0      -0.425\n",
      "2438                47                0      -0.207\n",
      "2439                41                0      -0.379\n",
      "2440                49                0      -0.267\n",
      "2441                41                1      -0.476\n",
      "2442                26                0      -0.196\n",
      "2443                61                2      -0.416\n",
      "2444                61                2      -0.208\n",
      "2445                61                2      -0.400\n",
      "2446                61                2      -0.202\n",
      "2447                36                0      -0.513\n",
      "2448                36                0      -0.194\n",
      "2449                47                1      -0.586\n",
      "2450                25                1      -0.845\n",
      "2451                10                1      -1.000\n",
      "2452                10                1      -0.615\n",
      "2453                47                1      -0.571\n",
      "2454                47                1      -0.349\n",
      "2455                35                1      -0.428\n",
      "2456                12                1       0.656\n",
      "2457                42                2      -0.547\n",
      "2458                49                2      -0.267\n",
      "2459                34                0      -0.726\n",
      "2460                39                0      -0.209\n",
      "2461                39                0      -0.200\n",
      "2462                36                0      -0.220\n",
      "2463                39                0      -0.216\n",
      "2464                34                0      -0.501\n",
      "\n",
      "[2465 rows x 3 columns]\n",
      "\n",
      "4.a Delete the extracted features\n",
      " DELETE http://localhost:5001/api/v0/feature-extraction/3b1e20c376624a7b9b524796125c457a\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\" 0. Load the test dataset\")\n",
    "    url = BASE_URL + '/datasets/{}'.format(dataset_name)\n",
    "    print(\" POST\", url)\n",
    "    res = requests.get(url).json()\n",
    "\n",
    "    # To use a custom dataset, simply specify the following variables\n",
    "    data_dir = res['data_dir']\n",
    "    relevant_files = res['seed_relevant_files']\n",
    "    non_relevant_files = res['seed_non_relevant_files']\n",
    "    ground_truth_file = res['ground_truth_file']  # (optional)\n",
    "\n",
    "\n",
    "    # 1. Feature extraction\n",
    "\n",
    "    print(\"\\n1.a Load dataset and initalize feature extraction\")\n",
    "    url = BASE_URL + '/feature-extraction'\n",
    "    print(\" POST\", url)\n",
    "    fe_opts = {'data_dir': data_dir,\n",
    "               'stop_words': 'english', 'chunk_size': 2000, 'n_jobs': -1,\n",
    "               'use_idf': 1, 'sublinear_tf': 0, 'binary': 0, 'n_features': 50001,\n",
    "               'analyzer': 'word', 'ngram_range': (1, 1), \"norm\": \"l2\"\n",
    "              }\n",
    "    res = requests.post(url, json=fe_opts).json()\n",
    "\n",
    "    dsid = res['id']\n",
    "    print(\"   => received {}\".format(list(res.keys())))\n",
    "    print(\"   => dsid = {}\".format(dsid))\n",
    "\n",
    "    print(\"\\n1.b Start feature extraction (in the background)\")\n",
    "\n",
    "    # Make this call in a background process (there should be a better way of doing it)\n",
    "    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n",
    "    print(\" POST\", url)\n",
    "    p = Process(target=requests.post, args=(url,))\n",
    "    p.start()\n",
    "    sleep(5.0) # wait a bit for the processing to start\n",
    "\n",
    "    print('\\n1.c Monitor feature extraction progress')\n",
    "    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n",
    "    print(\" GET\", url)\n",
    "\n",
    "    t0 = time()\n",
    "    while True:\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 520:\n",
    "            p.terminate()\n",
    "            raise ValueError('Processing did not start')\n",
    "        elif res.status_code == 200:\n",
    "            break # processing finished\n",
    "        data = res.json()\n",
    "        print('     ... {}k/{}k files processed in {:.1f} min'.format(\n",
    "                    data['n_samples_processed']//1000, data['n_samples']//1000, (time() - t0)/60.))\n",
    "        sleep(15.0)\n",
    "\n",
    "    p.terminate()  # just in case, should not be necessary\n",
    "\n",
    "\n",
    "    print(\"\\n1.d. check the parameters of the extracted features\")\n",
    "    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n",
    "    print(' GET', url)\n",
    "    res = requests.get(url).json()\n",
    "\n",
    "    print('\\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \\\n",
    "                                                      if \"filenames\" not in key]))\n",
    "\n",
    "\n",
    "    # 2. Document categorization with ML algorithms\n",
    "\n",
    "    print(\"\\n2.a. Train the ML categorization model\")\n",
    "    print(\"       {} relevant, {} non-relevant files\".format(\n",
    "        len(relevant_files), len(non_relevant_files)))\n",
    "    url = BASE_URL + '/categorization/'\n",
    "    print(\" POST\", url)\n",
    "    print(' Training...')\n",
    "\n",
    "    res = requests.post(url,\n",
    "                        json={'relevant_filenames': relevant_files,\n",
    "                              'non_relevant_filenames': non_relevant_files,\n",
    "                              'dataset_id': dsid,\n",
    "                              'method': 'LinearSVC',  # one of \"LinearSVC\", \"LogisticRegression\", 'xgboost'\n",
    "                              'cv': 0                          # Cross Validation\n",
    "                              }).json()\n",
    "\n",
    "    mid = res['id']\n",
    "    print(\"     => model id = {}\".format(mid))\n",
    "    print('    => Training scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n",
    "\n",
    "    print(\"\\n2.b. Check the parameters used in the categorization model\")\n",
    "    url = BASE_URL + '/categorization/{}'.format(mid)\n",
    "    print(\" GET\", url)\n",
    "    res = requests.get(url).json()\n",
    "\n",
    "    print('\\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \\\n",
    "                                                      if \"filenames\" not in key]))\n",
    "\n",
    "    print(\"\\n2.c Categorize the complete dataset with this model\")\n",
    "    url = BASE_URL + '/categorization/{}/predict'.format(mid)\n",
    "    print(\" GET\", url)\n",
    "    res = requests.get(url).json()\n",
    "    prediction = res['prediction']\n",
    "\n",
    "    print(\"    => Predicting {} relevant and {} non relevant documents\".format(\n",
    "        len(list(filter(lambda x: x>0, prediction))),\n",
    "        len(list(filter(lambda x: x<0, prediction)))))\n",
    "\n",
    "    print(\"\\n2.d Test categorization accuracy\")\n",
    "    print(\"         using {}\".format(ground_truth_file))  \n",
    "    url = BASE_URL + '/categorization/{}/test'.format(mid)\n",
    "    print(\"POST\", url)\n",
    "    res = requests.post(url, json={'ground_truth_filename': ground_truth_file}).json()\n",
    "\n",
    "    print('    => Test scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n",
    "\n",
    "\n",
    "    # 3. Document categorization with LSI\n",
    "\n",
    "    print(\"\\n3.a. Calculate LSI\")\n",
    "\n",
    "    url = BASE_URL + '/lsi/'\n",
    "    print(\"POST\", url)\n",
    "\n",
    "    n_components = 100\n",
    "    res = requests.post(url,\n",
    "                        json={'n_components': n_components,\n",
    "                              'dataset_id': dsid\n",
    "                              }).json()\n",
    "\n",
    "    lid = res['id']\n",
    "    print('  => LSI model id = {}'.format(lid))\n",
    "    print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(\n",
    "                            n_components, res['explained_variance']*100))\n",
    "    print(\"\\n3.b. Predict categorization with LSI\")\n",
    "\n",
    "    url = BASE_URL + '/lsi/{}/predict'.format(lid)\n",
    "    print(\"POST\", url)\n",
    "    res = requests.post(url,\n",
    "                        json={'relevant_filenames': relevant_files,\n",
    "                              'non_relevant_filenames': non_relevant_files\n",
    "                              }).json()\n",
    "    prediction = res['prediction']\n",
    "\n",
    "    print('    => Training scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n",
    "    df = pd.DataFrame({key: res[key] for key in res if 'prediction'==key or 'nearest' in key})\n",
    "\n",
    "\n",
    "    print(\"\\n3.c. Test categorization with LSI\")\n",
    "    url = BASE_URL + '/lsi/{}/test'.format(lid)\n",
    "    print(\" POST\", url)\n",
    "\n",
    "    res = requests.post(url,\n",
    "                        json={'relevant_filenames': relevant_files,\n",
    "                              'non_relevant_filenames': non_relevant_files,\n",
    "                              'ground_truth_filename': ground_truth_file\n",
    "                              }).json()\n",
    "    print('    => Test scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n",
    "\n",
    "    print('\\n', df)\n",
    "\n",
    "\n",
    "    print(\"\\n4.a Delete the extracted features\")\n",
    "    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n",
    "    print(\" DELETE\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:freediscovery-env]",
   "language": "python",
   "name": "conda-env-freediscovery-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
