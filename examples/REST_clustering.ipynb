{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nClustering Example [REST API]\n-----------------------------\n\nCluster documents into clusters\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import os.path\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport requests\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\ndef repr_clustering(labels, terms):\n    out = []\n    for ridx, row in enumerate(terms):\n        out.append({'cluster_names': row, 'N_documents': (labels == ridx).sum()})\n    out = pd.DataFrame(out).sort_values('N_documents', ascending=False)\n    return out\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n\nprint(\" 0. Load the example dataset\")\nurl = BASE_URL + '/example-dataset/{}'.format(dataset_name)\nprint(\" GET\", url)\ninput_ds = requests.get(url).json()\n\n# To use a custom dataset, simply specify the following variables\ndata_dir = input_ds['metadata']['data_dir']\ndataset_definition = [{'document_id': row['document_id'],\n                       'file_path': os.path.join(data_dir, row['file_path'])} \\\n                               for row in input_ds['dataset']]\n\n# # 1. Feature extraction (non hashed)\n\nprint(\"\\n1.a Load dataset and initalize feature extraction\")\nurl = BASE_URL + '/feature-extraction'\nprint(\" POST\", url)\nfe_opts = {'dataset_definition': dataset_definition,\n           'use_idf': 1, 'n_features': 30001,\n           'min_df': 4, 'max_df': 0.75 # filter out unfrequent / too frequent words\n           }\nres = requests.post(url, json=fe_opts).json()\n\ndsid = res['id']\nprint(\"   => received {}\".format(list(res.keys())))\nprint(\"   => dsid = {}\".format(dsid))\n\n\nprint(\"\\n1.b Run feature extraction\")\n# progress status is available for the hashed version only\nurl = BASE_URL+'/feature-extraction/{}'.format(dsid)\nprint(\" POST\", url)\nres = requests.post(url)\n\nprint(\"\\n1.d. check the parameters of the extracted features\")\nurl = BASE_URL + '/feature-extraction/{}'.format(dsid)\nprint(' GET', url)\nres = requests.get(url).json()\n\nprint('\\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \\\n                                                  if \"filenames\" not in key]))\n\nprint(\"\\n2. Calculate LSI\")\n\nurl = BASE_URL + '/lsi/'\nprint(\"POST\", url)\n\nn_components = 100\nres = requests.post(url,\n                    json={'n_components': n_components,\n                          'parent_id': dsid\n                          }).json()\n\nlsi_id = res['id']\nprint('  => LSI model id = {}'.format(lsi_id))\nprint('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(\n                        n_components, res['explained_variance']*100))\n\n# # 3. Document Clustering (LSI + K-Means)\n\nprint(\"\\n3.a. Document clustering (LSI + K-means)\")\n\nurl = BASE_URL + '/clustering/k-mean/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url,\n                    json={'parent_id': lsi_id,\n                          'n_clusters': 10,\n                          }).json()\n\nmid = res['id']\nprint(\"     => model id = {}\".format(mid))\n\nprint(\"\\n3.b. Computing cluster labels\")\nurl = BASE_URL + '/clustering/k-mean/{}'.format(mid)\nprint(\" GET\", url)\nres = requests.get(url,\n                   json={'n_top_words': 6\n                         }).json()\nt1 = time()\n\nprint('    .. computed in {:.1f}s'.format(t1 - t0))\nprint(repr_clustering(np.array(res['labels']), res['cluster_terms']))\n\n\n# # 4. Document Clustering (LSI + Ward Hierarchical Clustering)\n\nprint(\"\\n4.a. Document clustering (LSI + Ward HC)\")\n\nurl = BASE_URL + '/clustering/ward_hc/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url,\n                    json={'parent_id': lsi_id,\n                          'n_clusters': 10,\n                          'n_neighbors': 5  # this is the connectivity constraint\n                          }).json()\n\nmid = res['id']\nprint(\"     => model id = {}\".format(mid))\n\nprint(\"\\n4.b. Computing cluster labels\")\nurl = BASE_URL + '/clustering/ward_hc/{}'.format(mid)\nprint(\" GET\", url)\nres = requests.get(url,\n                   json={'n_top_words': 6\n                         }).json()\nt1 = time()\n\nprint('    .. computed in {:.1f}s'.format(t1 - t0))\nprint(repr_clustering(np.array(res['labels']), res['cluster_terms']))\n\n\n# 4. Cleaning\nprint(\"\\n5. Delete the extracted features\")\nurl = BASE_URL + '/feature-extraction/{}'.format(dsid)\nprint(\" DELETE\", url)\nrequests.delete(url)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}